# L2.1 - Sistema Modular de Ingesta de Documentos RAG

## Contexto del Proyecto

**Proyecto**: Community SaaS (Next.js 15 + Supabase)  
**Estado actual**: Sistema básico de upload y extracción funcionando  
**Objetivo**: Evolucionar a sistema RAG completo con arquitectura modular  
**Base de datos**: Supabase PostgreSQL con pgvector para embeddings  
**Experiencia previa**: Proyecto RAG completo validado en `/proyectos/RAG/`

## Análisis de Viabilidad Técnica

### ✅ PLAN APROBADO - Altamente Viable

El plan propuesto es técnicamente sólido y factible por:

1. **Base tecnológica establecida**: Sistema de extracción PDF ya funcionando
2. **Experiencia validada**: Proyecto RAG demuestra dominio completo del pipeline
3. **Arquitectura modular**: Diseño sigue patrones probados y escalables
4. **Integración natural**: Se alinea con Next.js + Supabase existente
5. **Flexibilidad real**: Procesos configurables según necesidad

## Arquitectura del Sistema Modular

### Estructura de Archivos Detallada

```
src/lib/ingesta/
├── core/
│   ├── types.ts              # Types e interfaces compartidos
│   ├── constants.ts          # Constantes del sistema
│   ├── errors.ts            # Error handling personalizado
│   └── pipeline.ts          # Orchestrator principal
├── modules/
│   ├── extraction/           # ✅ EXTRACCIÓN DE TEXTO
│   │   ├── index.ts         # Exports principales
│   │   ├── types.ts         # Types específicos
│   │   ├── pdfTextExtraction.ts    # PDF editable ✅ IMPLEMENTADO
│   │   ├── ocrExtraction.ts        # OCR wrapper (Google Vision) ✅ IMPLEMENTADO
│   │   ├── llmExtraction.ts        # LLM directo (Gemini) [FUTURO]
│   │   └── extractionOrchestrator.ts  # Strategy pattern [FUTURO]
│   ├── classification/       # ✅ CLASIFICACIÓN DE DOCUMENTOS
│   │   ├── index.ts         # Exports principales ✅ IMPLEMENTADO
│   │   ├── types.ts         # Types específicos ✅ IMPLEMENTADO
│   │   └── documentClassifier.ts   # IA classification ✅ IMPLEMENTADO
│   ├── metadata/            # 🆕 EXTRACCIÓN DE METADATOS
│   │   ├── contracts/       # 📋 Contratos de metadatos por tipo
│   │   │   ├── types.ts     # Tipos base comunes ✅ IMPLEMENTADO
│   │   │   ├── actaContract.ts        # Contrato ACTA ✅ IMPLEMENTADO
│   │   │   ├── contratoContract.ts    # Contrato CONTRATO [FUTURO]
│   │   │   ├── facturaContract.ts     # Contrato FACTURA [FUTURO]
│   │   │   ├── comunicadoContract.ts  # Contrato COMUNICADO [FUTURO]
│   │   │   └── index.ts     # Exporta todos los contratos ✅ IMPLEMENTADO
│   │   ├── extractors/      # 🤖 Extractores de metadatos OVERVIEW (nivel documento)
│   │   │   ├── types.ts     # Tipos para extractors
│   │   │   ├── actaMetadataExtractor.ts      # Metadatos globales ACTA (Gemini)
│   │   │   ├── contratoMetadataExtractor.ts  # Metadatos globales CONTRATO [FUTURO]
│   │   │   ├── facturaMetadataExtractor.ts   # Metadatos globales FACTURA [FUTURO]
│   │   │   ├── comunicadoMetadataExtractor.ts # Metadatos básicos COMUNICADO [FUTURO]
│   │   │   └── index.ts     # Exporta todos los extractors
│   │   ├── chunkers/        # ✂️ Generadores de chunks con metadatos específicos
│   │   │   ├── types.ts     # Tipos para chunkers
│   │   │   ├── actaChunker.ts         # Divide ACTA en chunks + metadatos chunk-específicos
│   │   │   ├── contratoChunker.ts     # Divide CONTRATO en chunks [FUTURO]
│   │   │   ├── facturaChunker.ts      # Divide FACTURA en chunks [FUTURO]
│   │   │   ├── comunicadoChunker.ts   # Divide COMUNICADO en chunks [FUTURO]
│   │   │   └── index.ts     # Exporta todos los chunkers
│   │   ├── metadataOrchestrator.ts  # 🎯 Orquestador principal (overview + chunking)
│   │   └── index.ts         # API pública del módulo
│   ├── processing/          # 🔄 PROCESAMIENTO Y CHUNKING
│   │   ├── index.ts
│   │   ├── types.ts
│   │   ├── chunker.ts             # Text chunking (adaptado del RAG)
│   │   ├── embedder.ts            # Embeddings (adaptado del RAG)
│   │   └── processingOrchestrator.ts
│   └── storage/             # 💾 ALMACENAMIENTO VECTORIAL
│       ├── index.ts
│       ├── types.ts
│       ├── vectorStore.ts         # Supabase + pgvector
│       ├── metadataStore.ts       # Metadata storage
│       └── storageOrchestrator.ts
├── processes/               # 🚀 PROCESOS CONFIGURABLES
│   ├── index.ts
│   ├── types.ts
│   ├── basicUpload.ts        # Proceso 1: Solo upload
│   ├── uploadExtract.ts      # Proceso 2: Upload + extracción
│   ├── uploadClassify.ts     # Proceso 3: Upload + extracción + clasificación
│   ├── uploadMetadata.ts     # Proceso 4: Upload + extracción + clasificación + metadata
│   └── fullRAGPipeline.ts    # Proceso 5: Pipeline completo RAG
├── config/
│   ├── index.ts
│   ├── moduleConfig.ts       # Configuraciones por módulo
│   ├── processConfig.ts      # Configuraciones por proceso
│   └── environmentConfig.ts # Variables de entorno
└── utils/
    ├── index.ts
    ├── validation.ts         # Validaciones
    ├── monitoring.ts         # Métricas y logging
    └── migration.ts          # Helpers para migrar del RAG
```

## **Arquitectura de Metadatos: Dos Niveles**

### **📄 NIVEL 1: Document Overview (Implementado)**
**Responsabilidad**: Metadatos globales de TODO el documento
```typescript
// UN solo conjunto de metadatos para el documento completo:
ActaMetadataStructure {
  chunk_type: "document_overview",
  lugar: "Las Rozas",
  tipo_reunion: "extraordinaria", 
  presidente_entrante: "Stefanía Nitto",
  topic_keywords: ["Piscina", "Balance", "Administracion"],
  decisiones_principales: ["Renovar piscina", "Aprobar presupuesto"]
}
```

**Módulos**:
- `actaContract.ts`: Define estructura y validaciones ✅
- `actaMetadataExtractor.ts`: Extrae metadatos globales con Gemini
- **Ventajas**: Simple, rápido, económico (~1000 tokens)
- **Uso**: Búsquedas, filtros, clasificación general

### **📋 NIVEL 2: Per-Chunk (Futuro)**
**Responsabilidad**: Metadatos específicos para CADA chunk/párrafo
```typescript
// Múltiples chunks, cada uno con sus metadatos específicos:
[
  ActaMetadataStructure { chunk_type: "content_chunk", topic_keywords: ["Piscina"], category: "tecnico" },
  ActaMetadataStructure { chunk_type: "content_chunk", topic_keywords: ["Balance"], category: "financiero" },  
  ActaMetadataStructure { chunk_type: "content_chunk", topic_keywords: ["Administracion"], category: "administrativo" }
]
```

**Módulos**:
- `actaChunker.ts`: Divide documento + extrae metadatos por chunk
- **Ventajas**: Búsqueda granular, mejor relevancia RAG
- **Desventajas**: Más complejo, más costoso (~5000 tokens)
- **Uso**: RAG search, respuestas contextuales específicas

### **🚀 Flujo de Procesamiento Completo**
```
PDF → pdfTextExtraction → documentClassifier → actaMetadataExtractor → [actaChunker] → Storage
      ↓                   ↓                    ↓                        ↓
   "texto..."         "acta"             metadatos overview      chunks individuales
                                         (SIEMPRE)               (OPCIONAL)
```

### **⚙️ Configuración Modular**
```typescript
ProcessConfig {
  extraction: true,        // Siempre extraer texto
  classification: true,    // Siempre clasificar tipo
  metadata_overview: true, // Siempre metadatos globales
  chunking: false         // Activar solo si necesitas RAG granular
}
```

### PROCESOS

1. Extracion de metadatos

- pdfTextExtraction ( si falla) ocrExtraction
  - documentClassifier (depende de pdfTextExtraction)
-

2. Extracion de metadatos y creacion de chunks

## Patrones de Diseño Implementados

### 1. Strategy Pattern

Para algoritmos intercambiables:

```typescript
interface ExtractionStrategy {
  extract(buffer: Buffer): Promise<ExtractionResult>;
}

class PDFParseStrategy implements ExtractionStrategy {}
class OCRStrategy implements ExtractionStrategy {}
class LLMStrategy implements ExtractionStrategy {}
```

### 2. Pipeline Pattern

Para orchestrar flujo de procesamiento:

```typescript
class DocumentPipeline {
  constructor(private modules: ModuleConfig[]) {}

  async execute(input: ProcessInput): Promise<ProcessResult> {
    let result = input;
    for (const module of this.modules) {
      result = await this.executeModule(module, result);
    }
    return result;
  }
}
```

### 3. Factory Pattern

Para crear instancias de módulos:

```typescript
class ModuleFactory {
  static create(type: ModuleType, config: ModuleConfig): ProcessingModule {
    switch (type) {
      case 'extraction':
        return new ExtractionModule(config);
      case 'classification':
        return new ClassificationModule(config);
      // ...
    }
  }
}
```

## Tipos y Contratos

### Core Types

```typescript
export interface ModuleConfig {
  name: string;
  version: string;
  enabled: boolean;
  parameters: Record<string, any>;
}

export interface ProcessResult {
  success: boolean;
  documentId?: string;
  error?: string;
  processingStatus: 'processing' | 'completed' | 'error';
  steps: ProcessSteps;
  metrics: ProcessMetrics;
}

export interface ProcessSteps {
  upload: boolean;
  extraction: boolean;
  classification: boolean;
  chunking: boolean;
  embedding: boolean;
  storage: boolean;
}
```

### Module Interfaces

```typescript
export interface ProcessingModule {
  configure(config: ModuleConfig): Promise<void>;
  process(input: any): Promise<any>;
  validate(input: any): Promise<boolean>;
  getMetrics(): ModuleMetrics;
}
```

## Integración con Supabase y pgvector

### Extensiones de Base de Datos

```sql
-- Habilitar pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Extender tabla documents existente
ALTER TABLE documents ADD COLUMN IF NOT EXISTS vector_status VARCHAR(20) DEFAULT 'pending';
ALTER TABLE documents ADD COLUMN IF NOT EXISTS chunk_count INTEGER DEFAULT 0;
ALTER TABLE documents ADD COLUMN IF NOT EXISTS embedding_model VARCHAR(100);

-- Nueva tabla para embeddings
CREATE TABLE document_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_content TEXT NOT NULL,
    chunk_metadata JSONB,
    embedding vector(768), -- Ajustar dimensión según modelo
    similarity_threshold FLOAT DEFAULT 0.8,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Índices para búsqueda vectorial eficiente
CREATE INDEX ON document_embeddings USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Índice para filtros por documento
CREATE INDEX ON document_embeddings (document_id, chunk_index);

-- Nueva tabla para metadatos semánticos
CREATE TABLE document_semantic_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    topics JSONB,
    entities JSONB,
    keywords JSONB,
    summary TEXT,
    confidence_score FLOAT,
    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### Storage Module Implementation

```typescript
export class VectorStore {
  constructor(private supabase: SupabaseClient) {}

  async storeEmbeddings(
    documentId: string,
    chunks: ChunkData[],
    embeddings: number[][]
  ): Promise<void> {
    const embeddingData = chunks.map((chunk, index) => ({
      document_id: documentId,
      chunk_index: index,
      chunk_content: chunk.content,
      chunk_metadata: chunk.metadata,
      embedding: embeddings[index],
    }));

    await this.supabase.from('document_embeddings').insert(embeddingData);
  }

  async searchSimilar(
    query: number[],
    limit: number = 10,
    threshold: number = 0.8
  ): Promise<SearchResult[]> {
    const { data, error } = await this.supabase.rpc('match_documents', {
      query_embedding: query,
      match_threshold: threshold,
      match_count: limit,
    });

    return data || [];
  }
}
```

## Procesos Configurables

### Proceso 1: Solo Upload

```typescript
export async function basicUploadProcess(
  formData: FormData
): Promise<ProcessResult> {
  const pipeline = new DocumentPipeline([
    ModuleFactory.create('upload', uploadConfig),
  ]);

  return pipeline.execute(formData);
}
```

### Proceso 4: Pipeline Completo RAG

```typescript
export async function fullRAGPipelineProcess(
  formData: FormData
): Promise<ProcessResult> {
  const pipeline = new DocumentPipeline([
    ModuleFactory.create('upload', uploadConfig),
    ModuleFactory.create('extraction', extractionConfig),
    ModuleFactory.create('classification', classificationConfig),
    ModuleFactory.create('processing', processingConfig),
    ModuleFactory.create('storage', storageConfig),
  ]);

  return pipeline.execute(formData);
}
```

## Migración del Proyecto RAG Existente

### Fase 1: Adaptar Módulos Core (Semana 1-2)

**Chunking Module Migration:**

```bash
# Del RAG: /core/chunking/chunking_module.py
# Al SaaS: /src/lib/ingesta/modules/processing/chunker.ts

# Estrategias a migrar:
- FixedSizeStrategy -> fixedSizeChunking.ts
- SemanticStrategy -> semanticChunking.ts
- LLMStrategy -> llmChunking.ts
```

**Embedding Module Migration:**

```bash
# Del RAG: /core/embedding/embedding_module.py
# Al SaaS: /src/lib/ingesta/modules/processing/embedder.ts

# Proveedores a migrar:
- GoogleEmbeddings -> googleEmbeddingsProvider.ts
- OpenAIEmbeddings -> openaiEmbeddingsProvider.ts
```

**Storage Module Migration:**

```bash
# Del RAG: /core/storage/storage_module.py
# Al SaaS: /src/lib/ingesta/modules/storage/vectorStore.ts

# Migrar de ChromaDB a Supabase + pgvector
```

### Fase 2: Integración con Next.js (Semana 3-4)

**Server Actions Integration:**

```typescript
'use server';

import { DocumentPipeline } from '@/lib/ingesta/core/pipeline';
import { ProcessType } from '@/lib/ingesta/core/types';

export async function processDocumentWithRAG(
  formData: FormData,
  processType: ProcessType = 'full'
): Promise<ProcessResult> {
  try {
    const pipeline = await DocumentPipeline.create(processType);
    return await pipeline.execute(formData);
  } catch (error) {
    return {
      success: false,
      error: error.message,
      processingStatus: 'error',
      steps: createEmptySteps(),
    };
  }
}
```

### Fase 3: UI y Configuración (Semana 5)

**Process Selection UI:**

```tsx
export function ProcessSelectionForm() {
  const [processType, setProcessType] = useState<ProcessType>('basic');

  return (
    <div className="space-y-4">
      <RadioGroup value={processType} onValueChange={setProcessType}>
        <div className="flex items-center space-x-2">
          <RadioGroupItem value="basic" id="basic" />
          <Label htmlFor="basic">Solo Upload</Label>
        </div>
        <div className="flex items-center space-x-2">
          <RadioGroupItem value="extract" id="extract" />
          <Label htmlFor="extract">Upload + Extracción</Label>
        </div>
        <div className="flex items-center space-x-2">
          <RadioGroupItem value="classify" id="classify" />
          <Label htmlFor="classify">Upload + Clasificación</Label>
        </div>
        <div className="flex items-center space-x-2">
          <RadioGroupItem value="full" id="full" />
          <Label htmlFor="full">Pipeline RAG Completo</Label>
        </div>
      </RadioGroup>

      <ProcessConfigEditor processType={processType} />
    </div>
  );
}
```

## Performance y Escalabilidad

### Optimizaciones Implementadas

1. **Procesamiento Asíncrono**

   - Background jobs para chunking pesado
   - Queue system para embeddings
   - Progress tracking en tiempo real

2. **Cache Inteligente**

   - Cache de embeddings por hash de contenido
   - Cache de clasificaciones por tipo de documento
   - Cache de metadatos extraídos

3. **Batch Processing**
   - Agrupación de operaciones vectoriales
   - Inserción en lotes para mejor performance
   - Paralelización de embeddings

### Monitoring y Métricas

```typescript
export interface ProcessMetrics {
  totalTime: number;
  moduleTimings: Record<string, number>;
  chunksGenerated: number;
  embeddingsCreated: number;
  storageOperations: number;
  cacheHits: number;
  errors: string[];
}

export class MetricsCollector {
  static collect(pipeline: DocumentPipeline): ProcessMetrics {
    return {
      totalTime: pipeline.getTotalExecutionTime(),
      moduleTimings: pipeline.getModuleTimings(),
      chunksGenerated: pipeline.getChunksCount(),
      embeddingsCreated: pipeline.getEmbeddingsCount(),
      storageOperations: pipeline.getStorageOpsCount(),
      cacheHits: pipeline.getCacheHits(),
      errors: pipeline.getErrors(),
    };
  }
}
```

## Gestión de Errores y Fallbacks

### Error Handling Strategy

```typescript
export class ProcessingError extends Error {
  constructor(
    public module: string,
    public stage: string,
    public originalError: Error,
    public recoverable: boolean = false
  ) {
    super(`${module}:${stage} - ${originalError.message}`);
  }
}

export class ErrorHandler {
  static async handleModuleError(
    error: ProcessingError,
    context: ProcessingContext
  ): Promise<ProcessingResult> {
    if (error.recoverable) {
      return await this.attemptRecovery(error, context);
    }

    return this.createErrorResult(error);
  }

  private static async attemptRecovery(
    error: ProcessingError,
    context: ProcessingContext
  ): Promise<ProcessingResult> {
    // Implementar estrategias de fallback específicas por módulo
    switch (error.module) {
      case 'extraction':
        return await this.fallbackExtraction(context);
      case 'classification':
        return await this.fallbackClassification(context);
      default:
        return this.createErrorResult(error);
    }
  }
}
```

## Testing Strategy

### Módulos de Test

```typescript
// test/modules/extraction.test.ts
describe('Extraction Module', () => {
  test('should extract text from PDF', async () => {
    const module = new ExtractionModule(testConfig);
    const result = await module.process(testPDFBuffer);

    expect(result.success).toBe(true);
    expect(result.text.length).toBeGreaterThan(0);
  });
});

// test/processes/fullPipeline.test.ts
describe('Full RAG Pipeline', () => {
  test('should process document end-to-end', async () => {
    const pipeline = await DocumentPipeline.create('full');
    const result = await pipeline.execute(testFormData);

    expect(result.success).toBe(true);
    expect(result.steps.embedding).toBe(true);
    expect(result.steps.storage).toBe(true);
  });
});
```

## Plan de Implementación

### Sprint 1: Core Infrastructure (2 semanas)

- [x] Análisis y documentación (este documento)
- [ ] Setup estructura de carpetas
- [ ] Implementar core types y interfaces
- [ ] Migrar extraction module básico
- [ ] Tests unitarios básicos

### Sprint 2: Module Migration (2 semanas)

- [ ] Migrar chunking module del RAG
- [ ] Migrar embedding module del RAG
- [ ] Adaptar storage para Supabase + pgvector
- [ ] Implementar pipeline orchestrator
- [ ] Tests de integración

### Sprint 3: Process Implementation (2 semanas)

- [ ] Implementar 4 procesos configurables
- [ ] Integración con Server Actions existentes
- [ ] UI para selección de procesos
- [ ] Sistema de configuración
- [ ] Tests end-to-end

### Sprint 4: Production Ready (1 semana)

- [ ] Error handling robusto
- [ ] Monitoring y métricas
- [ ] Performance optimizations
- [ ] Documentation completa
- [ ] Deploy y validación

## Conclusiones

### Beneficios del Sistema Modular

1. **Flexibilidad**: Procesos adaptables según necesidad
2. **Mantenibilidad**: Módulos independientes y testeables
3. **Escalabilidad**: Arquitectura preparada para crecimiento
4. **Reutilización**: Aprovechamiento del código RAG validado
5. **Performance**: Optimizaciones específicas por módulo

### Próximos Pasos

1. **Validar con el equipo** este plan arquitectónico
2. **Definir prioridades** de implementación según necesidad
3. **Setup del entorno** de desarrollo con las nuevas estructuras
4. **Comenzar migración** con módulos más críticos
5. **Implementar testing** desde el inicio para garantizar calidad

### Riesgos Identificados

1. **Complejidad de migración**: Mitigado con enfoque incremental
2. **Performance de pgvector**: Mitigado con índices adecuados
3. **Integración con código existente**: Mitigado con interfaces estándar

Este sistema modular representa una evolución natural del proyecto, aprovechando la experiencia del RAG existente mientras se integra perfectamente con la arquitectura Next.js + Supabase actual.

---

**Documento generado**: 2025-09-13  
**Versión**: 1.0  
**Estado**: Propuesta aprobada para implementación
